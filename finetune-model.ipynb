{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import interpreter_login\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv(\"/Users/wongyenchik/Documents/GitHub/finetune-mlx-math-model/.env\")\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_KEY\")\n",
    "region_name = os.getenv(\"AWS_REGION\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the OpenAI API key\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "import subprocess\n",
    "import boto3\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"HuggingFaceH4/MATH-500\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$',\n",
       " 'solution': 'We have that $r = \\\\sqrt{0^2 + 3^2} = 3.$  Also, if we draw the line connecting the origin and $(0,3),$ this line makes an angle of $\\\\frac{\\\\pi}{2}$ with the positive $x$-axis.\\n\\n[asy]\\nunitsize(0.8 cm);\\n\\ndraw((-0.5,0)--(3.5,0));\\ndraw((0,-0.5)--(0,3.5));\\ndraw(arc((0,0),3,0,90),red,Arrow(6));\\n\\ndot((0,3), red);\\nlabel(\"$(0,3)$\", (0,3), W);\\ndot((3,0), red);\\n[/asy]\\n\\nTherefore, the polar coordinates are $\\\\boxed{\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)}.$',\n",
       " 'answer': '\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'] = dataset['test'].remove_columns(['subject', 'level', 'unique_id'])\n",
    "\n",
    "# Check the result to see if the 'input' column is removed\n",
    "dataset['test'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_test = 100\n",
    "num_val = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_list = []\n",
    "response_list = []\n",
    "\n",
    "for line in dataset['test']:\n",
    "    instruction_list.append(line['problem'])\n",
    "    response_list.append(line['solution'] + \" The answer is:\" + line['answer'] + \" -MathGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We count the number of times $\\\\frac{1}{n^3}$ appears in the sum\\n\\\\[\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3},\\\\]where $n$ is a fixed positive integer.  (In other words, we are conditioning the sum on $j + k$.)  We get a term of $\\\\frac{1}{n^3}$ each time $j + k = n.$  The pairs $(j,k)$ that work are $(1,n - 1),$ $(2,n - 2),$ $\\\\dots,$ $(n - 1,1),$ for a total of $n - 1$ pairs.  Therefore,\\n\\\\begin{align*}\\n\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3} &= \\\\sum_{n = 1}^\\\\infty \\\\frac{n - 1}{n^3} \\\\\\\\\\n&= \\\\sum_{n = 1}^\\\\infty \\\\left( \\\\frac{n}{n^3} - \\\\frac{1}{n^3} \\\\right) \\\\\\\\\\n&= \\\\sum_{n = 1}^\\\\infty \\\\left( \\\\frac{1}{n^2} - \\\\frac{1}{n^3} \\\\right) \\\\\\\\\\n&= \\\\sum_{n = 1}^\\\\infty \\\\frac{1}{n^2} - \\\\sum_{n = 1}^\\\\infty \\\\frac{1}{n^3} \\\\\\\\\\n&= \\\\boxed{p - q}.\\n\\\\end{align*} The answer is:p - q -MathGPT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instruction_list[0]\n",
    "response_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<s>[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. It reacts to feedback aptly and ends responses with its signature \\'MathGPT\\'. MathGPT will tailor the length of its responses to match the student\\'s questions, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\\n\\nPlease respond to the following comment.\\n \\nAltitudes $\\\\overline{AD}$ and $\\\\overline{BE}$ of $\\\\triangle ABC$ intersect at $H$.  If $\\\\angle BAC = 54^\\\\circ$ and $\\\\angle ABC = 52^\\\\circ$, then what is $\\\\angle AHB$? \\n[/INST]\\nFirst, we build a diagram:\\n\\n[asy]\\n\\nsize(150); defaultpen(linewidth(0.8));\\n\\npair B = (0,0), C = (3,0), A = (1.8,2), P = foot(A,B,C), Q = foot(B,A,C),H = intersectionpoint(B--Q,A--P);\\n\\ndraw(A--B--C--cycle);\\n\\ndraw(A--P^^B--Q);\\n\\nlabel(\"$A$\",A,N); label(\"$B$\",B,W); label(\"$C$\",C,E); label(\"$D$\",P,S); label(\"$E$\",Q,E); label(\"$H$\",H,NW);\\n\\ndraw(rightanglemark(C,P,H,3.5));\\n\\ndraw(rightanglemark(H,Q,C,3.5));\\n\\n[/asy]\\n\\nWe have $\\\\angle AHB = \\\\angle DHE$, and from quadrilateral $CDHE$, we have  \\\\begin{align*}\\n\\\\angle DHE &= 360^\\\\circ - \\\\angle HEC - \\\\angle ECD - \\\\angle CDH \\\\\\\\\\n&= 360^\\\\circ - 90^\\\\circ - \\\\angle ACB - 90^\\\\circ\\\\\\\\\\n&= 180^\\\\circ - \\\\angle ACB.\\n\\\\end{align*}From triangle $ABC$, we have $180^\\\\circ - \\\\angle ACB = \\\\angle BAC + \\\\angle ABC = 54^\\\\circ + 52^\\\\circ = \\\\boxed{106^\\\\circ}$. The answer is:106^\\\\circ -MathGPT</s>'}\n"
     ]
    }
   ],
   "source": [
    "# prompt format\n",
    "intstructions_string = f\"\"\"MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. \\\n",
    "It reacts to feedback aptly and ends responses with its signature 'MathGPT'. \\\n",
    "MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "\n",
    "example_template = lambda comment, response: f'''<s>[INST] {intstructions_string} \\n{comment} \\n[/INST]\\n''' + response + \"</s>\"\n",
    "\n",
    "example_list = []\n",
    "for i in range(len(instruction_list)):\n",
    "    example = {\"text\":example_template(instruction_list[i],response_list[i])}\n",
    "    example_list.append(example)\n",
    "\n",
    "print(example_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test and val data\n",
    "test_val_index_list = random.sample(range(0, len(example_list)-1), num_test+num_val)\n",
    "\n",
    "test_list = [example_list[index] for index in test_val_index_list[:num_test]]\n",
    "val_list = [example_list[index] for index in test_val_index_list[num_test:]]\n",
    "\n",
    "for example in test_list+val_list:\n",
    "    example_list.remove(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store train, test and valid data as jsonl as MLX model required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "with open('data/train.jsonl', 'w') as outfile:\n",
    "    for example in example_list:\n",
    "        json.dump(example, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test.jsonl', 'w') as outfile:\n",
    "    for example in test_list:\n",
    "        json.dump(example, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/valid.jsonl', 'w') as outfile:\n",
    "    for example in val_list:\n",
    "        json.dump(example, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/train.jsonl' uploaded successfully to 'finetune-model-layer/data/train.jsonl'\n",
      "File 'data/test.jsonl' uploaded successfully to 'finetune-model-layer/data/test.jsonl'\n",
      "File 'data/valid.jsonl' uploaded successfully to 'finetune-model-layer/data/valid.jsonl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Upload function\n",
    "def upload_to_s3(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket inside a specific folder\"\"\"\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    s3.upload_file(file_name, bucket, object_name)\n",
    "    print(f\"File '{file_name}' uploaded successfully to '{bucket}/{object_name}'\")\n",
    "    return True\n",
    "\n",
    "# Local file path\n",
    "file_names = ['data/train.jsonl', 'data/test.jsonl', 'data/valid.jsonl']\n",
    "\n",
    "# S3 bucket name and S3 folder\n",
    "bucket_name = 'finetune-model-layer'\n",
    "s3_folder = 'data'  # This is the folder in the S3 bucket\n",
    "\n",
    "for file_name in file_names:\n",
    "    # Specify the full S3 object name (key)\n",
    "    object_name = f\"{s3_folder}/{file_name.split('/')[-1]}\"\n",
    "\n",
    "    # Upload the file to S3\n",
    "    upload_to_s3(file_name, bucket_name, object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████████████████████| 9/9 [04:00<00:00, 26.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Quantizing\n",
      "[INFO] Quantized model with 4.501 bits per weight.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'mlx_lm.convert', '--hf-path', 'google/gemma-2-2b-it', '-q'], returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"google/gemma-2-2b-it\"\n",
    "command = [\n",
    "    'python', '-m', 'mlx_lm.convert', '--hf-path', model , '-q'\n",
    "]\n",
    "subprocess.run(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lora layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.038% (0.983M/2614.342M)\n",
      "Starting training..., iters: 1\n",
      "Iter 1: Val loss 1.994, Val took 296.700s\n",
      "Iter 1: Train loss 1.946, Learning Rate 1.000e-05, It/sec 0.314, Tokens/sec 58.051, Trained Tokens 1850, Peak mem 16.309 GB\n",
      "Saved final weights to adapters/adapters.safetensors.\n",
      "Testing\n",
      "Test loss 2.035, Test ppl 7.650.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define variables\n",
    "model = \"mlx_model\"\n",
    "iters = 1\n",
    "steps_per_eval = 20\n",
    "val_batches = -1\n",
    "learning_rate = 1e-5\n",
    "adapter_path = \"adapters\"\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "# Define MLflow experiment\n",
    "mlflow.set_experiment(\"MLX LoRA Fine-Tuning\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"MLX-Model-LoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"):\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"model\", model)\n",
    "    mlflow.log_param(\"iters\", iters)\n",
    "    mlflow.log_param(\"steps_per_eval\", steps_per_eval)\n",
    "    mlflow.log_param(\"val_batches\", val_batches)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"adapter_path\", adapter_path)\n",
    "\n",
    "    # Create the command list\n",
    "    command = [\n",
    "        'python', '-m', 'mlx_lm.lora', '--model', model, '--train',\n",
    "        '--iters', str(iters),\n",
    "        '--steps-per-eval', str(steps_per_eval),\n",
    "        '--val-batches', str(val_batches),\n",
    "        '--learning-rate', str(learning_rate),\n",
    "        '--test', \n",
    "        '--adapter-path', adapter_path\n",
    "    ]\n",
    "    \n",
    "    # Run the subprocess command to execute fine-tuning\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store lora layer at s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'adapters/adapters.safetensors' uploaded successfully to 'finetune-model-layer/lora_layer/adapters.safetensors'\n",
      "File 'adapters/adapter_config.json' uploaded successfully to 'finetune-model-layer/lora_layer/adapter_config.json'\n"
     ]
    }
   ],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Upload function\n",
    "def upload_to_s3(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket inside a specific folder\"\"\"\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    s3.upload_file(file_name, bucket, object_name)\n",
    "    print(f\"File '{file_name}' uploaded successfully to '{bucket}/{object_name}'\")\n",
    "    return True\n",
    "\n",
    "# Local file path\n",
    "folder_name = 'adapters'\n",
    "\n",
    "# S3 bucket name and S3 folder\n",
    "bucket_name = 'finetune-model-layer'\n",
    "s3_folder = 'lora_layer'  # This is the folder in the S3 bucket\n",
    "\n",
    "# Loop through all files in the adapters folder\n",
    "for root, dirs, files in os.walk(folder_name):\n",
    "    for file in files:\n",
    "        # Get the full file path\n",
    "        file_path = os.path.join(root, file)\n",
    "        \n",
    "        # Create the object name by preserving the folder structure\n",
    "        object_name = os.path.join(s3_folder, os.path.relpath(file_path, folder_name)).replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        # Upload the file to S3\n",
    "        upload_to_s3(file_path, bucket_name, object_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the ai (human evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt format\n",
    "intstructions_string = f\"\"\"MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. \\\n",
    "It reacts to feedback aptly and ends responses with its signature 'MathGPT'. \\\n",
    "MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = lambda comment: f'''<s>[INST] {intstructions_string} \\n{comment} \\n[/INST]\\n'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Let's break this down step-by-step:\n",
      "\n",
      "**1. Understand the Exchange Rate:**\n",
      "\n",
      "* We know that 1200 lire in Italy was equal to $1.50 in the U.S. \n",
      "* This means the exchange rate between lire and dollars is 1200 lire = $1.50\n",
      "\n",
      "**2.  Set up the Equation:**\n",
      "\n",
      "* We want to find out how many dollars are equivalent to 1,000,000 lire.\n",
      "*  Let 'x' represent the number of dollars.\n",
      "*  We can set up the equation:  x = 1,000,000 lire * (Dollar amount per lire)\n",
      "\n",
      "**3.  Solve for 'x':**\n",
      "\n",
      "*  We know that 1200 lire = $1.50\n",
      "*  We can use this information to find the dollar amount per lire.\n",
      "*  Divide the dollar amount by the number of lire:  $1.50 / 1200 lire = $0.00125 per lire\n",
      "*  Now, multiply this value by 1,000,000 to find the equivalent dollar amount:  x = 1,000,000 lire * $0.00125 per lire \n",
      "*  Calculate: x = $1,250\n",
      "\n",
      "**MathGPT:**  The equivalent dollar amount for 1,000,000 lire is $1,250. \n",
      "\n",
      "==========\n",
      "Prompt: 184 tokens, 252.990 tokens-per-sec\n",
      "Generation: 332 tokens, 18.077 tokens-per-sec\n",
      "Peak memory: 1.826 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'mlx_lm.generate', '--model', 'mlx_model', '--adapter-path', 'adapters', '--max-tokens', '512', '--prompt', \"<s>[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. It reacts to feedback aptly and ends responses with its signature 'MathGPT'. MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\\n\\nPlease respond to the following comment.\\n \\nIn 1992, a scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $\\\\$1.50$ in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? \\n[/INST]\\n\"], returncode=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define variables\n",
    "comment = \"In 1992, a scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $\\\\$1.50$ in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire?\"\n",
    "\n",
    "# Generate the prompt using the comment\n",
    "prompt = prompt_template(comment)\n",
    "\n",
    "# Create the command list\n",
    "command = [\n",
    "    'python', '-m', 'mlx_lm.generate',\n",
    "    '--model', 'mlx_model',\n",
    "    '--adapter-path', 'adapters',\n",
    "    '--max-tokens', '512',\n",
    "    '--prompt', prompt\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Let's break this down step-by-step:\n",
      "\n",
      "**1. Understand the Exchange Rate:**\n",
      "\n",
      "* We know that 1200 lire in Italy was equal to $1.50 in the U.S. \n",
      "* This means the exchange rate between lire and dollars is 1200 lire = $1.50\n",
      "\n",
      "**2.  Set up the Equation:**\n",
      "\n",
      "* We want to find out how many dollars are equivalent to 1,000,000 lire.\n",
      "*  Let 'x' represent the number of dollars.\n",
      "*  We can set up the equation:  x = 1,000,000 lire * (Dollar amount per lire)\n",
      "\n",
      "**3.  Solve for 'x':**\n",
      "\n",
      "*  We know that 1200 lire = $1.50\n",
      "*  Therefore, 1,000,000 lire = $1,000,000 * (Dollar amount per lire)\n",
      "*  We can solve for the dollar amount per lire by dividing both sides by 1,000,000:  (Dollar amount per lire) = $1,000,000 / 1,000,000 = $1.00\n",
      "\n",
      "\n",
      "**4.  Calculate the Dollars:**\n",
      "\n",
      "*  We can now use the equation:  x = 1,000,000 lire * $1.00\n",
      "*  Therefore, x = $1,000,000\n",
      "\n",
      "\n",
      "**MathGPT:**  So, 1,000,000 lire is equivalent to $1,000,000 dollars. \n",
      "\n",
      "==========\n",
      "Prompt: 184 tokens, 258.549 tokens-per-sec\n",
      "Generation: 371 tokens, 21.006 tokens-per-sec\n",
      "Peak memory: 1.833 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'mlx_lm.generate', '--model', 'mlx_model', '--max-tokens', '512', '--prompt', \"<s>[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. It reacts to feedback aptly and ends responses with its signature 'MathGPT'. MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\\n\\nPlease respond to the following comment.\\n \\nIn 1992, a scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $\\\\$1.50$ in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? \\n[/INST]\\n\"], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables\n",
    "comment = \"In 1992, a scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $\\\\$1.50$ in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire?\"\n",
    "\n",
    "# Generate the prompt using the comment\n",
    "prompt = prompt_template(comment)\n",
    "\n",
    "# Create the command list\n",
    "command = [\n",
    "    'python', '-m', 'mlx_lm.generate',\n",
    "    '--model', 'mlx_model',\n",
    "    '--max-tokens', '512',\n",
    "    '--prompt', prompt\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and push to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo created: https://huggingface.co/yenchik/mlx-gemma-2-2b-it-math\n",
      "Found 8 candidate files to upload\n",
      "Recovering from metadata files: 100%|██████████| 8/8 [00:00<00:00, 1237.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "---------- 2025-03-06 10:43:52 (0:00:00) ----------\n",
      "Files:   hashed 3/8 (48.7K/1.5G) | pre-uploaded: 0/0 (0.0/1.5G) (+8 unsure) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 5 | get upload mode: 1 | pre-uploading: 0 | committing: 0 | waiting: 0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   3%|▌                 | 47.7M/1.47G [00:55<15:10, 1.56MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:44:52 (0:01:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   7%|█▍                 | 110M/1.47G [01:55<21:25, 1.06MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:45:53 (0:02:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  11%|██                 | 162M/1.47G [02:55<18:54, 1.15MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:46:53 (0:03:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  16%|███▏                | 232M/1.47G [03:55<23:53, 864kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:47:53 (0:04:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  20%|███▋               | 290M/1.47G [04:56<17:18, 1.14MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:48:53 (0:05:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  24%|████▋              | 358M/1.47G [05:56<13:39, 1.36MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:49:53 (0:06:01) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  30%|█████▉              | 439M/1.47G [06:56<20:43, 830kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:50:54 (0:07:01) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  33%|██████▌             | 482M/1.47G [07:56<25:18, 651kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:51:54 (0:08:01) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  37%|██████▉            | 539M/1.47G [08:57<12:02, 1.29MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:52:54 (0:09:01) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  39%|███████▊            | 572M/1.47G [09:57<18:18, 819kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:53:54 (0:10:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  43%|████████▏          | 636M/1.47G [10:57<12:35, 1.11MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:54:54 (0:11:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  46%|█████████▎          | 681M/1.47G [11:57<16:00, 823kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:55:55 (0:12:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  50%|██████████          | 743M/1.47G [12:57<12:17, 988kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:56:55 (0:13:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  54%|██████████▎        | 798M/1.47G [13:58<09:52, 1.14MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:57:55 (0:14:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  58%|███████████▋        | 858M/1.47G [14:58<15:06, 676kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:58:55 (0:15:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  62%|████████████▍       | 915M/1.47G [15:58<09:37, 964kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:59:55 (0:16:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  66%|█████████████▏      | 970M/1.47G [16:58<09:17, 899kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:00:56 (0:17:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  70%|████████████▌     | 1.02G/1.47G [17:58<06:16, 1.19MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:01:56 (0:18:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  73%|█████████████▏    | 1.08G/1.47G [18:58<04:24, 1.49MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:02:56 (0:19:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  77%|█████████████▉    | 1.14G/1.47G [19:59<02:24, 2.31MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:03:56 (0:20:04) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  82%|██████████████▋   | 1.20G/1.47G [20:59<02:27, 1.81MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:04:57 (0:21:04) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  87%|███████████████▋  | 1.29G/1.47G [21:59<02:50, 1.08MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:05:57 (0:22:04) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  91%|████████████████▎ | 1.34G/1.47G [22:59<02:10, 1.03MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:06:57 (0:23:04) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  94%|█████████████████▉ | 1.38G/1.47G [24:00<01:42, 836kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:07:57 (0:24:05) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  98%|█████████████████▋| 1.45G/1.47G [25:00<00:14, 1.72MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:08:57 (0:25:05) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|███████████████████| 1.47G/1.47G [25:25<00:00, 964kB/s]\n",
      "Removing 6 file(s) from commit that have not changed.\n",
      "All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "\n",
      "---------- 2025-03-06 11:09:35 (0:25:42) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 3/3 (1.5G/1.5G) | committed: 8/8 (1.5G/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 0 | waiting: 0\n",
      "---------------------------------------------------\n",
      "INFO:huggingface_hub._upload_large_folder:\n",
      "---------- 2025-03-06 11:09:35 (0:25:42) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 3/3 (1.5G/1.5G) | committed: 8/8 (1.5G/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 0 | waiting: 0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful, go to https://huggingface.co/yenchik/mlx-gemma-2-2b-it-math for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'mlx_lm.fuse', '--model', 'mlx_model', '--save-path', 'my-model', '--adapter-path', 'adapters', '--upload-repo', 'yenchik/mlx-gemma-2-2b-it-math', '--hf-path', 'google/gemma-2-2b-it'], returncode=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables\n",
    "save_path = \"my-model\"\n",
    "\n",
    "# Create the command list\n",
    "command = [\n",
    "    'python', '-m', 'mlx_lm.fuse',\n",
    "    '--model', 'mlx_model',\n",
    "    '--save-path', save_path,\n",
    "    '--adapter-path', 'adapters',\n",
    "    '--upload-repo', 'yenchik/mlx-gemma-2-2b-it-math',\n",
    "    '--hf-path', 'google/gemma-2-2b-it'\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the response with openai and store in MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>We plug in $x = 4$: \\begin{align*}\\n3(4) + 2y ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>By Cauchy-Schwarz,\\n\\[(a^2 + b^2 + c^2 + d^2)(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>First, we factor $10!:$\\n\\begin{align*} 10!&amp;=1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>Rewriting $\\frac{4321}{5^7\\cdot2^8}$ as a deci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>In general, to express the number $0.\\overline...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>Completing the square gives us $(x - 5)^2 + (y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>Since $y$ and $\\sqrt{x}$ are inversely proport...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>In isosceles right triangle $\\triangle ABC$ be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>\\begin{align*}\\n\\dbinom{31}{28} &amp;= \\dbinom{31}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>We count the number of ways that some lane can...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  MathGPT, functioning as a virtual Math chatbot...   \n",
       "1  MathGPT, functioning as a virtual Math chatbot...   \n",
       "2  MathGPT, functioning as a virtual Math chatbot...   \n",
       "3  MathGPT, functioning as a virtual Math chatbot...   \n",
       "4  MathGPT, functioning as a virtual Math chatbot...   \n",
       "5  MathGPT, functioning as a virtual Math chatbot...   \n",
       "6  MathGPT, functioning as a virtual Math chatbot...   \n",
       "7  MathGPT, functioning as a virtual Math chatbot...   \n",
       "8  MathGPT, functioning as a virtual Math chatbot...   \n",
       "9  MathGPT, functioning as a virtual Math chatbot...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  We plug in $x = 4$: \\begin{align*}\\n3(4) + 2y ...  \n",
       "1  By Cauchy-Schwarz,\\n\\[(a^2 + b^2 + c^2 + d^2)(...  \n",
       "2  First, we factor $10!:$\\n\\begin{align*} 10!&=1...  \n",
       "3  Rewriting $\\frac{4321}{5^7\\cdot2^8}$ as a deci...  \n",
       "4  In general, to express the number $0.\\overline...  \n",
       "5  Completing the square gives us $(x - 5)^2 + (y...  \n",
       "6  Since $y$ and $\\sqrt{x}$ are inversely proport...  \n",
       "7  In isosceles right triangle $\\triangle ABC$ be...  \n",
       "8  \\begin{align*}\\n\\dbinom{31}{28} &= \\dbinom{31}...  \n",
       "9  We count the number of ways that some lane can...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the test dataset (without the response)\n",
    "inputs = []\n",
    "ground_truth = []\n",
    "\n",
    "# Go through each example in test_list\n",
    "for example in test_list:\n",
    "    # Extract the comment part from the 'text' field (everything between [INST] and [/INST])\n",
    "    inst_start = example['text'].find(\"[INST]\") + len(\"[INST]\")\n",
    "    inst_end = example['text'].find(\"[/INST]\")\n",
    "\n",
    "    # Extract the comment (instruction) and the response\n",
    "    questions = example['text'][inst_start:inst_end].strip()  # This is the prompt for the test data\n",
    "    response = example['text'][inst_end + len(\"[/INST]\"):-4].strip()  # This is the corresponding answer\n",
    "\n",
    "    # Append to inputs and ground truth\n",
    "    inputs.append(questions)\n",
    "    ground_truth.append(response)\n",
    "\n",
    "# Convert inputs and ground_truth to a DataFrame\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": inputs,        \n",
    "        \"ground_truth\": ground_truth\n",
    "    }\n",
    ")\n",
    "\n",
    "# Example to see the eval_data\n",
    "eval_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the structure of MLX model in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "import subprocess\n",
    "\n",
    "class MLXModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        # Store the model path, which will be used later in the prediction step\n",
    "        self.model_path = context.artifacts[\"model\"]\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Log the input type\n",
    "        print(f\"Input type: {type(model_input)}\")\n",
    "        print(f\"Input data: {model_input}\")\n",
    "\n",
    "        # Check if the input is a DataFrame and extract the 'inputs' column\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            if 'inputs' in model_input.columns:\n",
    "                # Loop through the DataFrame and process each row\n",
    "                responses = []\n",
    "                for comment in model_input['inputs']:\n",
    "                    prompt = self.prompt_template(comment)\n",
    "\n",
    "                    command = [\n",
    "                        'python', '-m', 'mlx_lm.generate',\n",
    "                        '--model', self.model_path,  # Use the stored model path from load_context\n",
    "                        '--max-tokens', '512',\n",
    "                        '--prompt', prompt\n",
    "                    ]\n",
    "\n",
    "                    # Run the command to generate the response\n",
    "                    result = subprocess.run(command, capture_output=True, text=True)\n",
    "                    responses.append(result.stdout)\n",
    "                return responses\n",
    "            else:\n",
    "                raise ValueError(\"Expected 'inputs' column in the DataFrame.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input type: {type(model_input)}\")\n",
    "\n",
    "    def prompt_template(self, comment):\n",
    "        # Custom prompt template\n",
    "        instructions_string = \"\"\"MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always providing the students steps to solve the question before providing the answer. \\\n",
    "It reacts to feedback aptly and ends responses with its signature 'MathGPT'. \\\n",
    "MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "        return f'''<s>[INST] {instructions_string} \\n{comment} \\n[/INST]\\n'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log the model in MLFlow and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 11:19:08 INFO mlflow.pyfunc: Validating input example against model signature\n",
      "Downloading artifacts: 100%|██████████| 25/25 [00:00<00:00, 41.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "0  [INST] MathGPT, functioning as a virtual Math ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:  94%|█████████▍| 30/32 [00:31<00:02,  1.06s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run angry-ox-725 at: http://127.0.0.1:5000/#/experiments/240149989688063106/runs/7f3a8e4464bd4964a60084da36f11e7a\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/240149989688063106\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "The following failures occurred while downloading one or more artifacts from http://127.0.0.1:5000/api/2.0/mlflow-artifacts/artifacts/240149989688063106/7f3a8e4464bd4964a60084da36f11e7a/artifacts:\n##### File model/artifacts/my-model/model.safetensors #####\n(\"Connection broken: InvalidChunkLength(got length b'HTTP/1.1 500 Internal Server Error\\\\r\\\\n', 0 bytes read)\", InvalidChunkLength(got length b'HTTP/1.1 500 Internal Server Error\\r\\n', 0 bytes read))\n##### File model/artifacts/my-model/tokenizer.json #####\nResponse ended prematurely",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMlflowException\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     25\u001b[39m logged_model_info = mlflow.pyfunc.log_model(\n\u001b[32m     26\u001b[39m     artifact_path=\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Path where the model will be stored in MLflow\u001b[39;00m\n\u001b[32m     27\u001b[39m     python_model=MLXModelWrapper(),  \u001b[38;5;66;03m# Your custom model wrapper\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     input_example=input_example\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Use the function to evaluate the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m results = \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogged_model_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mground_truth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion-answering\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43manswer_correctness\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlatency\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Example output DataFrame for inspection\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/mlflow/models/evaluation/base.py:1685\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, data, model_type, targets, predictions, dataset_path, feature_names, evaluators, evaluator_config, custom_metrics, extra_metrics, custom_artifacts, validation_thresholds, baseline_model, env_manager, model_config, baseline_config, inference_params)\u001b[39m\n\u001b[32m   1683\u001b[39m         model = _get_model_from_deployment_endpoint_uri(model, inference_params)\n\u001b[32m   1684\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m         model = \u001b[43m_load_model_or_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1686\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m env_manager != _EnvManager.LOCAL:\n\u001b[32m   1687\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m   1688\u001b[39m         message=\u001b[33m\"\u001b[39m\u001b[33mThe model argument must be a string URI referring to an MLflow model when a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1689\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnon-local env_manager is specified.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1690\u001b[39m         error_code=INVALID_PARAMETER_VALUE,\n\u001b[32m   1691\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/mlflow/pyfunc/__init__.py:1230\u001b[39m, in \u001b[36m_load_model_or_server\u001b[39m\u001b[34m(model_uri, env_manager, model_config)\u001b[39m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyfunc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscoring_server\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m   1225\u001b[39m     ScoringServerClient,\n\u001b[32m   1226\u001b[39m     StdinScoringServerClient,\n\u001b[32m   1227\u001b[39m )\n\u001b[32m   1229\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env_manager == _EnvManager.LOCAL:\n\u001b[32m-> \u001b[39m\u001b[32m1230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1232\u001b[39m _logger.info(\u001b[33m\"\u001b[39m\u001b[33mStarting model server for model environment restoration.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1234\u001b[39m local_path = _download_artifact_from_uri(artifact_uri=model_uri)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/mlflow/tracing/provider.py:412\u001b[39m, in \u001b[36mtrace_disabled.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m disable()\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     is_func_called, result = \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    414\u001b[39m     enable()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/mlflow/pyfunc/__init__.py:1089\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_uri, suppress_warnings, dst_path, model_config)\u001b[39m\n\u001b[32m   1085\u001b[39m         entity_list.append(Entity(job=job_entity))\n\u001b[32m   1087\u001b[39m     lineage_header_info = LineageHeaderInfo(entities=entity_list) \u001b[38;5;28;01mif\u001b[39;00m entity_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m local_path = \u001b[43m_download_artifact_from_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m    \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineage_header_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineage_header_info\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m suppress_warnings:\n\u001b[32m   1094\u001b[39m     model_requirements = _get_pip_requirements_from_model_path(local_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/mlflow/tracking/artifact_utils.py:116\u001b[39m, in \u001b[36m_download_artifact_from_uri\u001b[39m\u001b[34m(artifact_uri, output_path, lineage_header_info)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repo, ModelsArtifactRepository):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m repo.download_artifacts(\n\u001b[32m    112\u001b[39m         artifact_path=artifact_path,\n\u001b[32m    113\u001b[39m         dst_path=output_path,\n\u001b[32m    114\u001b[39m         lineage_header_info=lineage_header_info,\n\u001b[32m    115\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/mlflow/store/artifact/runs_artifact_repo.py:131\u001b[39m, in \u001b[36mRunsArtifactRepository.download_artifacts\u001b[39m\u001b[34m(self, artifact_path, dst_path)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_artifacts\u001b[39m(\u001b[38;5;28mself\u001b[39m, artifact_path, dst_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    115\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[33;03m    Download an artifact file or directory to a local directory if applicable, and return a\u001b[39;00m\n\u001b[32m    117\u001b[39m \u001b[33;03m    local path for it.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m \u001b[33;03m        Absolute path of the local filesystem location containing the desired artifacts.\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_artifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/mlflow/store/artifact/artifact_repo.py:314\u001b[39m, in \u001b[36mArtifactRepository.download_artifacts\u001b[39m\u001b[34m(self, artifact_path, dst_path)\u001b[39m\n\u001b[32m    308\u001b[39m         template = \u001b[33m\"\u001b[39m\u001b[33m##### File \u001b[39m\u001b[38;5;132;01m{path}\u001b[39;00m\u001b[33m #####\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{error}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    310\u001b[39m     failures = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    311\u001b[39m         template.format(path=path, error=error, traceback=tracebacks[path])\n\u001b[32m    312\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m path, error \u001b[38;5;129;01min\u001b[39;00m failed_downloads.items()\n\u001b[32m    313\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m    315\u001b[39m         message=(\n\u001b[32m    316\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe following failures occurred while downloading one or more\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    317\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m artifacts from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.artifact_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_truncate_error(failures)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    318\u001b[39m         )\n\u001b[32m    319\u001b[39m     )\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m os.path.join(dst_path, artifact_path)\n",
      "\u001b[31mMlflowException\u001b[39m: The following failures occurred while downloading one or more artifacts from http://127.0.0.1:5000/api/2.0/mlflow-artifacts/artifacts/240149989688063106/7f3a8e4464bd4964a60084da36f11e7a/artifacts:\n##### File model/artifacts/my-model/model.safetensors #####\n(\"Connection broken: InvalidChunkLength(got length b'HTTP/1.1 500 Internal Server Error\\\\r\\\\n', 0 bytes read)\", InvalidChunkLength(got length b'HTTP/1.1 500 Internal Server Error\\r\\n', 0 bytes read))\n##### File model/artifacts/my-model/tokenizer.json #####\nResponse ended prematurely"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics import latency\n",
    "from mlflow.metrics.genai import answer_correctness\n",
    "import time\n",
    "import mlflow\n",
    "signature = mlflow.models.infer_signature(\n",
    "    model_input=pd.DataFrame({\n",
    "    'inputs': [\n",
    "        \"[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always providing steps before the answer. ...\",\n",
    "    ]\n",
    "}),\n",
    "    model_output=[\"==========\\nLet's solve that!  \\n\\nTo find the answer, we simply add the numbers together: 4 + 4 = 8. \\n\\nSo, 4 + 4 = 8. \\n\\nMathGPT \\n\\n==========\\nPrompt: 119 tokens, 102.358 tokens-per-sec\\nGeneration: 49 tokens, 15.268 tokens-per-sec\\nPeak memory: 1.713 GB\\n\",\n",
    " \"==========\\nTo find the derivative of x², we use the power rule of differentiation.  \\n\\nThe power rule states that the derivative of x<sup>n</sup> is nx<sup>n-1</sup>.\\n\\nApplying this to x²:\\n\\n*  The derivative of x² is 2x<sup>2-1</sup> \\n*  Simplifying, we get 2x<sup>1</sup> \\n*  Therefore, the derivative of x² is **2x**. \\n\\n\\nMathGPT \\n\\n==========\\nPrompt: 120 tokens, 118.289 tokens-per-sec\\nGeneration: 99 tokens, 18.342 tokens-per-sec\\nPeak memory: 1.714 GB\\n\"],\n",
    ")\n",
    "input_example = pd.DataFrame({\n",
    "    'inputs': [\n",
    "        \"[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always providing steps before the answer. ...\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "# The evaluation process remains the same\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "mlflow.set_experiment(\"LLM Evaluation\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    logged_model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",  # Path where the model will be stored in MLflow\n",
    "        python_model=MLXModelWrapper(),  # Your custom model wrapper\n",
    "        artifacts={\"model\": \"/Users/wongyenchik/Documents/GitHub/finetune-mlx-math-model/my-model\"},  # The path to your saved MLX model\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "\n",
    "    # Use the function to evaluate the model\n",
    "    results = mlflow.evaluate(\n",
    "        logged_model_info.model_uri,\n",
    "        data=eval_data[:10],\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        extra_metrics=[\n",
    "            answer_correctness(),\n",
    "            latency(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Example output DataFrame for inspection\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load(\"yenchik/mlx-gemma-2-2b-it-math\")\n",
    "\n",
    "prompt = \"In 1992, a scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $\\\\$1.50$ in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire?\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
