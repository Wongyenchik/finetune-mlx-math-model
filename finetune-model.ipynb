{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import interpreter_login\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "load_dotenv(\"/Users/wongyenchik/Documents/GitHub/finetune-mlx-math-model/.env\")\n",
    "aws_access_key_id = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "aws_secret_access_key = os.getenv(\"AWS_SECRET_KEY\")\n",
    "region_name = os.getenv(\"AWS_REGION\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the OpenAI API key\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "import subprocess\n",
    "import boto3\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_dataset_name = \"HuggingFaceH4/MATH-500\"\n",
    "dataset = load_dataset(huggingface_dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$',\n",
       " 'solution': 'We have that $r = \\\\sqrt{0^2 + 3^2} = 3.$  Also, if we draw the line connecting the origin and $(0,3),$ this line makes an angle of $\\\\frac{\\\\pi}{2}$ with the positive $x$-axis.\\n\\n[asy]\\nunitsize(0.8 cm);\\n\\ndraw((-0.5,0)--(3.5,0));\\ndraw((0,-0.5)--(0,3.5));\\ndraw(arc((0,0),3,0,90),red,Arrow(6));\\n\\ndot((0,3), red);\\nlabel(\"$(0,3)$\", (0,3), W);\\ndot((3,0), red);\\n[/asy]\\n\\nTherefore, the polar coordinates are $\\\\boxed{\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)}.$',\n",
       " 'answer': '\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'] = dataset['test'].remove_columns(['subject', 'level', 'unique_id'])\n",
    "\n",
    "# Check the result to see if the 'input' column is removed\n",
    "dataset['test'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_test = 100\n",
    "num_val = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_list = []\n",
    "response_list = []\n",
    "\n",
    "for line in dataset['test']:\n",
    "    instruction_list.append(line['problem'])\n",
    "    response_list.append(line['solution'] + \" The answer is:\" + line['answer'] + \" -MathGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We count the number of times $\\\\frac{1}{n^3}$ appears in the sum\\n\\\\[\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3},\\\\]where $n$ is a fixed positive integer.  (In other words, we are conditioning the sum on $j + k$.)  We get a term of $\\\\frac{1}{n^3}$ each time $j + k = n.$  The pairs $(j,k)$ that work are $(1,n - 1),$ $(2,n - 2),$ $\\\\dots,$ $(n - 1,1),$ for a total of $n - 1$ pairs.  Therefore,\\n\\\\begin{align*}\\n\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3} &= \\\\sum_{n = 1}^\\\\infty \\\\frac{n - 1}{n^3} \\\\\\\\\\n&= \\\\sum_{n = 1}^\\\\infty \\\\left( \\\\frac{n}{n^3} - \\\\frac{1}{n^3} \\\\right) \\\\\\\\\\n&= \\\\sum_{n = 1}^\\\\infty \\\\left( \\\\frac{1}{n^2} - \\\\frac{1}{n^3} \\\\right) \\\\\\\\\\n&= \\\\sum_{n = 1}^\\\\infty \\\\frac{1}{n^2} - \\\\sum_{n = 1}^\\\\infty \\\\frac{1}{n^3} \\\\\\\\\\n&= \\\\boxed{p - q}.\\n\\\\end{align*} The answer is:p - q -MathGPT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instruction_list[0]\n",
    "response_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<s>[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. It reacts to feedback aptly and ends responses with its signature \\'MathGPT\\'. MathGPT will tailor the length of its responses to match the student\\'s questions, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\\n\\nPlease respond to the following comment.\\n \\nAltitudes $\\\\overline{AD}$ and $\\\\overline{BE}$ of $\\\\triangle ABC$ intersect at $H$.  If $\\\\angle BAC = 54^\\\\circ$ and $\\\\angle ABC = 52^\\\\circ$, then what is $\\\\angle AHB$? \\n[/INST]\\nFirst, we build a diagram:\\n\\n[asy]\\n\\nsize(150); defaultpen(linewidth(0.8));\\n\\npair B = (0,0), C = (3,0), A = (1.8,2), P = foot(A,B,C), Q = foot(B,A,C),H = intersectionpoint(B--Q,A--P);\\n\\ndraw(A--B--C--cycle);\\n\\ndraw(A--P^^B--Q);\\n\\nlabel(\"$A$\",A,N); label(\"$B$\",B,W); label(\"$C$\",C,E); label(\"$D$\",P,S); label(\"$E$\",Q,E); label(\"$H$\",H,NW);\\n\\ndraw(rightanglemark(C,P,H,3.5));\\n\\ndraw(rightanglemark(H,Q,C,3.5));\\n\\n[/asy]\\n\\nWe have $\\\\angle AHB = \\\\angle DHE$, and from quadrilateral $CDHE$, we have  \\\\begin{align*}\\n\\\\angle DHE &= 360^\\\\circ - \\\\angle HEC - \\\\angle ECD - \\\\angle CDH \\\\\\\\\\n&= 360^\\\\circ - 90^\\\\circ - \\\\angle ACB - 90^\\\\circ\\\\\\\\\\n&= 180^\\\\circ - \\\\angle ACB.\\n\\\\end{align*}From triangle $ABC$, we have $180^\\\\circ - \\\\angle ACB = \\\\angle BAC + \\\\angle ABC = 54^\\\\circ + 52^\\\\circ = \\\\boxed{106^\\\\circ}$. The answer is:106^\\\\circ -MathGPT</s>'}\n"
     ]
    }
   ],
   "source": [
    "# prompt format\n",
    "intstructions_string = f\"\"\"MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. \\\n",
    "It reacts to feedback aptly and ends responses with its signature 'MathGPT'. \\\n",
    "MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "\n",
    "example_template = lambda comment, response: f'''<s>[INST] {intstructions_string} \\n{comment} \\n[/INST]\\n''' + response + \"</s>\"\n",
    "\n",
    "example_list = []\n",
    "for i in range(len(instruction_list)):\n",
    "    example = {\"text\":example_template(instruction_list[i],response_list[i])}\n",
    "    example_list.append(example)\n",
    "\n",
    "print(example_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test and val data\n",
    "test_val_index_list = random.sample(range(0, len(example_list)-1), num_test+num_val)\n",
    "\n",
    "test_list = [example_list[index] for index in test_val_index_list[:num_test]]\n",
    "val_list = [example_list[index] for index in test_val_index_list[num_test:]]\n",
    "\n",
    "for example in test_list+val_list:\n",
    "    example_list.remove(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store train, test and valid data as jsonl as MLX model required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "with open('data/train.jsonl', 'w') as outfile:\n",
    "    for example in example_list:\n",
    "        json.dump(example, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test.jsonl', 'w') as outfile:\n",
    "    for example in test_list:\n",
    "        json.dump(example, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/valid.jsonl', 'w') as outfile:\n",
    "    for example in val_list:\n",
    "        json.dump(example, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'data/train.jsonl' uploaded successfully to 'finetune-model-layer/data/train.jsonl'\n",
      "File 'data/test.jsonl' uploaded successfully to 'finetune-model-layer/data/test.jsonl'\n",
      "File 'data/valid.jsonl' uploaded successfully to 'finetune-model-layer/data/valid.jsonl'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Upload function\n",
    "def upload_to_s3(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket inside a specific folder\"\"\"\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    s3.upload_file(file_name, bucket, object_name)\n",
    "    print(f\"File '{file_name}' uploaded successfully to '{bucket}/{object_name}'\")\n",
    "    return True\n",
    "\n",
    "# Local file path\n",
    "file_names = ['data/train.jsonl', 'data/test.jsonl', 'data/valid.jsonl']\n",
    "\n",
    "# S3 bucket name and S3 folder\n",
    "bucket_name = 'finetune-model-layer'\n",
    "s3_folder = 'data'  # This is the folder in the S3 bucket\n",
    "\n",
    "for file_name in file_names:\n",
    "    # Specify the full S3 object name (key)\n",
    "    object_name = f\"{s3_folder}/{file_name.split('/')[-1]}\"\n",
    "\n",
    "    # Upload the file to S3\n",
    "    upload_to_s3(file_name, bucket_name, object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 9 files: 100%|██████████████████████████| 9/9 [04:00<00:00, 26.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Quantizing\n",
      "[INFO] Quantized model with 4.501 bits per weight.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'mlx_lm.convert', '--hf-path', 'google/gemma-2-2b-it', '-q'], returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"google/gemma-2-2b-it\"\n",
    "command = [\n",
    "    'python', '-m', 'mlx_lm.convert', '--hf-path', model , '-q'\n",
    "]\n",
    "subprocess.run(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lora layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n",
      "Loading datasets\n",
      "Training\n",
      "Trainable parameters: 0.038% (0.983M/2614.342M)\n",
      "Starting training..., iters: 1\n",
      "Iter 1: Val loss 1.994, Val took 296.700s\n",
      "Iter 1: Train loss 1.946, Learning Rate 1.000e-05, It/sec 0.314, Tokens/sec 58.051, Trained Tokens 1850, Peak mem 16.309 GB\n",
      "Saved final weights to adapters/adapters.safetensors.\n",
      "Testing\n",
      "Test loss 2.035, Test ppl 7.650.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define variables\n",
    "model = \"mlx_model\"\n",
    "iters = 1\n",
    "steps_per_eval = 20\n",
    "val_batches = -1\n",
    "learning_rate = 1e-5\n",
    "adapter_path = \"adapters\"\n",
    "\n",
    "\n",
    "import subprocess\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "# Define MLflow experiment\n",
    "mlflow.set_experiment(\"MLX LoRA Fine-Tuning\")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"MLX-Model-LoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"):\n",
    "\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"model\", model)\n",
    "    mlflow.log_param(\"iters\", iters)\n",
    "    mlflow.log_param(\"steps_per_eval\", steps_per_eval)\n",
    "    mlflow.log_param(\"val_batches\", val_batches)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"adapter_path\", adapter_path)\n",
    "\n",
    "    # Create the command list\n",
    "    command = [\n",
    "        'python', '-m', 'mlx_lm.lora', '--model', model, '--train',\n",
    "        '--iters', str(iters),\n",
    "        '--steps-per-eval', str(steps_per_eval),\n",
    "        '--val-batches', str(val_batches),\n",
    "        '--learning-rate', str(learning_rate),\n",
    "        '--test', \n",
    "        '--adapter-path', adapter_path\n",
    "    ]\n",
    "    \n",
    "    # Run the subprocess command to execute fine-tuning\n",
    "    subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store lora layer at s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'adapters/adapters.safetensors' uploaded successfully to 'finetune-model-layer/lora_layer/adapters.safetensors'\n",
      "File 'adapters/adapter_config.json' uploaded successfully to 'finetune-model-layer/lora_layer/adapter_config.json'\n"
     ]
    }
   ],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=aws_access_key_id,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    region_name=region_name\n",
    ")\n",
    "\n",
    "# Upload function\n",
    "def upload_to_s3(file_name, bucket, object_name=None):\n",
    "    \"\"\"Upload a file to an S3 bucket inside a specific folder\"\"\"\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    s3.upload_file(file_name, bucket, object_name)\n",
    "    print(f\"File '{file_name}' uploaded successfully to '{bucket}/{object_name}'\")\n",
    "    return True\n",
    "\n",
    "# Local file path\n",
    "folder_name = 'adapters'\n",
    "\n",
    "# S3 bucket name and S3 folder\n",
    "bucket_name = 'finetune-model-layer'\n",
    "s3_folder = 'lora_layer'  # This is the folder in the S3 bucket\n",
    "\n",
    "# Loop through all files in the adapters folder\n",
    "for root, dirs, files in os.walk(folder_name):\n",
    "    for file in files:\n",
    "        # Get the full file path\n",
    "        file_path = os.path.join(root, file)\n",
    "        \n",
    "        # Create the object name by preserving the folder structure\n",
    "        object_name = os.path.join(s3_folder, os.path.relpath(file_path, folder_name)).replace(\"\\\\\", \"/\")\n",
    "        \n",
    "        # Upload the file to S3\n",
    "        upload_to_s3(file_path, bucket_name, object_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the ai (human evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt format\n",
    "intstructions_string = f\"\"\"MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. \\\n",
    "It reacts to feedback aptly and ends responses with its signature 'MathGPT'. \\\n",
    "MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = lambda comment: f'''<s>[INST] {intstructions_string} \\n{comment} \\n[/INST]\\n'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Let's break this down step-by-step:\n",
      "\n",
      "**1. Understand the Exchange Rate:**\n",
      "\n",
      "* We know that 1200 lire in Italy was equal to $1.50 in the U.S. \n",
      "* This means the exchange rate between lire and dollars is 1200 lire = $1.50\n",
      "\n",
      "**2.  Set up the Equation:**\n",
      "\n",
      "* We want to find out how many dollars are equivalent to 1,000,000 lire.\n",
      "*  Let 'x' represent the number of dollars.\n",
      "*  We can set up the equation:  x = 1,000,000 lire * (Dollar amount per lire)\n",
      "\n",
      "**3.  Solve for 'x':**\n",
      "\n",
      "*  We know that 1200 lire = $1.50\n",
      "*  We can use this information to find the dollar amount per lire.\n",
      "*  Divide the dollar amount by the number of lire:  $1.50 / 1200 lire = $0.00125 per lire\n",
      "*  Now, multiply this value by 1,000,000 to find the equivalent dollar amount:  x = 1,000,000 lire * $0.00125 per lire \n",
      "*  Calculate: x = $1,250\n",
      "\n",
      "**MathGPT:**  The equivalent dollar amount for 1,000,000 lire is $1,250. \n",
      "\n",
      "==========\n",
      "Prompt: 184 tokens, 252.990 tokens-per-sec\n",
      "Generation: 332 tokens, 18.077 tokens-per-sec\n",
      "Peak memory: 1.826 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'mlx_lm.generate', '--model', 'mlx_model', '--adapter-path', 'adapters', '--max-tokens', '512', '--prompt', \"<s>[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. It reacts to feedback aptly and ends responses with its signature 'MathGPT'. MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\\n\\nPlease respond to the following comment.\\n \\nIn 1992, a scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $\\\\$1.50$ in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? \\n[/INST]\\n\"], returncode=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define variables\n",
    "comment = \"In 1992, a scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $\\\\$1.50$ in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire?\"\n",
    "\n",
    "# Generate the prompt using the comment\n",
    "prompt = prompt_template(comment)\n",
    "\n",
    "# Create the command list\n",
    "command = [\n",
    "    'python', '-m', 'mlx_lm.generate',\n",
    "    '--model', 'mlx_model',\n",
    "    '--adapter-path', 'adapters',\n",
    "    '--max-tokens', '512',\n",
    "    '--prompt', prompt\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Let's break this down step-by-step:\n",
      "\n",
      "**1. Understand the Exchange Rate:**\n",
      "\n",
      "* We know that 1200 lire in Italy was equal to $1.50 in the U.S. \n",
      "* This means the exchange rate between lire and dollars is 1200 lire = $1.50\n",
      "\n",
      "**2.  Set up the Equation:**\n",
      "\n",
      "* We want to find out how many dollars are equivalent to 1,000,000 lire.\n",
      "*  Let 'x' represent the number of dollars.\n",
      "*  We can set up the equation:  x = 1,000,000 lire * (Dollar amount per lire)\n",
      "\n",
      "**3.  Solve for 'x':**\n",
      "\n",
      "*  We know that 1200 lire = $1.50\n",
      "*  Therefore, 1,000,000 lire = $1,000,000 * (Dollar amount per lire)\n",
      "*  We can solve for the dollar amount per lire by dividing both sides by 1,000,000:  (Dollar amount per lire) = $1,000,000 / 1,000,000 = $1.00\n",
      "\n",
      "\n",
      "**4.  Calculate the Dollars:**\n",
      "\n",
      "*  We can now use the equation:  x = 1,000,000 lire * $1.00\n",
      "*  Therefore, x = $1,000,000\n",
      "\n",
      "\n",
      "**MathGPT:**  So, 1,000,000 lire is equivalent to $1,000,000 dollars. \n",
      "\n",
      "==========\n",
      "Prompt: 184 tokens, 258.549 tokens-per-sec\n",
      "Generation: 371 tokens, 21.006 tokens-per-sec\n",
      "Peak memory: 1.833 GB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'mlx_lm.generate', '--model', 'mlx_model', '--max-tokens', '512', '--prompt', \"<s>[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always provide the students steps to solve the question before provide answer. It reacts to feedback aptly and ends responses with its signature 'MathGPT'. MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\\n\\nPlease respond to the following comment.\\n \\nIn 1992, a scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $\\\\$1.50$ in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire? \\n[/INST]\\n\"], returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables\n",
    "comment = \"In 1992, a scoop of gelato could be purchased in Italy for 1200 lire. The same gelato would have cost $\\\\$1.50$ in the U.S. At the equivalent exchange rate between the lire and the dollar, how many dollars would be equivalent to 1,000,000 lire?\"\n",
    "\n",
    "# Generate the prompt using the comment\n",
    "prompt = prompt_template(comment)\n",
    "\n",
    "# Create the command list\n",
    "command = [\n",
    "    'python', '-m', 'mlx_lm.generate',\n",
    "    '--model', 'mlx_model',\n",
    "    '--max-tokens', '512',\n",
    "    '--prompt', prompt\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and push to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo created: https://huggingface.co/yenchik/mlx-gemma-2-2b-it-math\n",
      "Found 8 candidate files to upload\n",
      "Recovering from metadata files: 100%|██████████| 8/8 [00:00<00:00, 1237.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "---------- 2025-03-06 10:43:52 (0:00:00) ----------\n",
      "Files:   hashed 3/8 (48.7K/1.5G) | pre-uploaded: 0/0 (0.0/1.5G) (+8 unsure) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 5 | get upload mode: 1 | pre-uploading: 0 | committing: 0 | waiting: 0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   3%|▌                 | 47.7M/1.47G [00:55<15:10, 1.56MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:44:52 (0:01:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:   7%|█▍                 | 110M/1.47G [01:55<21:25, 1.06MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:45:53 (0:02:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  11%|██                 | 162M/1.47G [02:55<18:54, 1.15MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:46:53 (0:03:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  16%|███▏                | 232M/1.47G [03:55<23:53, 864kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:47:53 (0:04:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  20%|███▋               | 290M/1.47G [04:56<17:18, 1.14MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:48:53 (0:05:00) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  24%|████▋              | 358M/1.47G [05:56<13:39, 1.36MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:49:53 (0:06:01) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  30%|█████▉              | 439M/1.47G [06:56<20:43, 830kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:50:54 (0:07:01) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  33%|██████▌             | 482M/1.47G [07:56<25:18, 651kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:51:54 (0:08:01) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  37%|██████▉            | 539M/1.47G [08:57<12:02, 1.29MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:52:54 (0:09:01) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  39%|███████▊            | 572M/1.47G [09:57<18:18, 819kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:53:54 (0:10:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  43%|████████▏          | 636M/1.47G [10:57<12:35, 1.11MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:54:54 (0:11:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  46%|█████████▎          | 681M/1.47G [11:57<16:00, 823kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:55:55 (0:12:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  50%|██████████          | 743M/1.47G [12:57<12:17, 988kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:56:55 (0:13:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  54%|██████████▎        | 798M/1.47G [13:58<09:52, 1.14MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:57:55 (0:14:02) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  58%|███████████▋        | 858M/1.47G [14:58<15:06, 676kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:58:55 (0:15:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  62%|████████████▍       | 915M/1.47G [15:58<09:37, 964kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 10:59:55 (0:16:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  66%|█████████████▏      | 970M/1.47G [16:58<09:17, 899kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:00:56 (0:17:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  70%|████████████▌     | 1.02G/1.47G [17:58<06:16, 1.19MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:01:56 (0:18:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  73%|█████████████▏    | 1.08G/1.47G [18:58<04:24, 1.49MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:02:56 (0:19:03) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  77%|█████████████▉    | 1.14G/1.47G [19:59<02:24, 2.31MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:03:56 (0:20:04) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  82%|██████████████▋   | 1.20G/1.47G [20:59<02:27, 1.81MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:04:57 (0:21:04) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  87%|███████████████▋  | 1.29G/1.47G [21:59<02:50, 1.08MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:05:57 (0:22:04) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  91%|████████████████▎ | 1.34G/1.47G [22:59<02:10, 1.03MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:06:57 (0:23:04) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  94%|█████████████████▉ | 1.38G/1.47G [24:00<01:42, 836kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:07:57 (0:24:05) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors:  98%|█████████████████▋| 1.45G/1.47G [25:00<00:14, 1.72MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K\u001b[F\n",
      "---------- 2025-03-06 11:08:57 (0:25:05) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 2/3 (38.6M/1.5G) | committed: 0/8 (0.0/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 1 | committing: 0 | waiting: 5\n",
      "---------------------------------------------------\n",
      "                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|███████████████████| 1.47G/1.47G [25:25<00:00, 964kB/s]\n",
      "Removing 6 file(s) from commit that have not changed.\n",
      "All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "All files have been processed! Exiting worker.\n",
      "INFO:huggingface_hub._upload_large_folder:All files have been processed! Exiting worker.\n",
      "\n",
      "---------- 2025-03-06 11:09:35 (0:25:42) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 3/3 (1.5G/1.5G) | committed: 8/8 (1.5G/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 0 | waiting: 0\n",
      "---------------------------------------------------\n",
      "INFO:huggingface_hub._upload_large_folder:\n",
      "---------- 2025-03-06 11:09:35 (0:25:42) ----------\n",
      "Files:   hashed 8/8 (1.5G/1.5G) | pre-uploaded: 3/3 (1.5G/1.5G) | committed: 8/8 (1.5G/1.5G) | ignored: 0\n",
      "Workers: hashing: 0 | get upload mode: 0 | pre-uploading: 0 | committing: 0 | waiting: 0\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful, go to https://huggingface.co/yenchik/mlx-gemma-2-2b-it-math for details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', '-m', 'mlx_lm.fuse', '--model', 'mlx_model', '--save-path', 'my-model', '--adapter-path', 'adapters', '--upload-repo', 'yenchik/mlx-gemma-2-2b-it-math', '--hf-path', 'google/gemma-2-2b-it'], returncode=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables\n",
    "save_path = \"my-model\"\n",
    "\n",
    "# Create the command list\n",
    "command = [\n",
    "    'python', '-m', 'mlx_lm.fuse',\n",
    "    '--model', 'mlx_model',\n",
    "    '--save-path', save_path,\n",
    "    '--adapter-path', 'adapters',\n",
    "    '--upload-repo', 'yenchik/mlx-gemma-2-2b-it-math',\n",
    "    '--hf-path', 'google/gemma-2-2b-it'\n",
    "]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the response with openai and store in MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>There are 100 numbers possible between 1 and 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>Six books are removed from the shelves, so $24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>Since everything in sight is even, we should b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>This looks like the equation of a circle, but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>If there are $b$ gold coins in each of the ori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>The probability that Dan wins is $\\frac12$. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>We have $f(5) = 2(5) -3 = 7$, so $g(f(5)-1) = ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>We will find the positive divisors of 14 by fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>Let Amy's, Ben's, and Chris's ages be $a$, $b$...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MathGPT, functioning as a virtual Math chatbot...</td>\n",
       "      <td>Since $\\sqrt{16}&lt;\\sqrt{20}&lt;\\sqrt{25}$, or, equ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  MathGPT, functioning as a virtual Math chatbot...   \n",
       "1  MathGPT, functioning as a virtual Math chatbot...   \n",
       "2  MathGPT, functioning as a virtual Math chatbot...   \n",
       "3  MathGPT, functioning as a virtual Math chatbot...   \n",
       "4  MathGPT, functioning as a virtual Math chatbot...   \n",
       "5  MathGPT, functioning as a virtual Math chatbot...   \n",
       "6  MathGPT, functioning as a virtual Math chatbot...   \n",
       "7  MathGPT, functioning as a virtual Math chatbot...   \n",
       "8  MathGPT, functioning as a virtual Math chatbot...   \n",
       "9  MathGPT, functioning as a virtual Math chatbot...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  There are 100 numbers possible between 1 and 1...  \n",
       "1  Six books are removed from the shelves, so $24...  \n",
       "2  Since everything in sight is even, we should b...  \n",
       "3  This looks like the equation of a circle, but ...  \n",
       "4  If there are $b$ gold coins in each of the ori...  \n",
       "5  The probability that Dan wins is $\\frac12$. Th...  \n",
       "6  We have $f(5) = 2(5) -3 = 7$, so $g(f(5)-1) = ...  \n",
       "7  We will find the positive divisors of 14 by fi...  \n",
       "8  Let Amy's, Ben's, and Chris's ages be $a$, $b$...  \n",
       "9  Since $\\sqrt{16}<\\sqrt{20}<\\sqrt{25}$, or, equ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the test dataset (without the response)\n",
    "inputs = []\n",
    "ground_truth = []\n",
    "\n",
    "# Go through each example in test_list\n",
    "for example in test_list:\n",
    "    # Extract the comment part from the 'text' field (everything between [INST] and [/INST])\n",
    "    inst_start = example['text'].find(\"[INST]\") + len(\"[INST]\")\n",
    "    inst_end = example['text'].find(\"[/INST]\")\n",
    "\n",
    "    # Extract the comment (instruction) and the response\n",
    "    questions = example['text'][inst_start:inst_end].strip()  # This is the prompt for the test data\n",
    "    response = example['text'][inst_end + len(\"[/INST]\"):-4].strip()  # This is the corresponding answer\n",
    "\n",
    "    # Append to inputs and ground truth\n",
    "    inputs.append(questions)\n",
    "    ground_truth.append(response)\n",
    "\n",
    "# Convert inputs and ground_truth to a DataFrame\n",
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": inputs,        \n",
    "        \"ground_truth\": ground_truth\n",
    "    }\n",
    ")\n",
    "\n",
    "# Example to see the eval_data\n",
    "eval_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the structure of MLX model in MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/finetune-model/lib/python3.13/site-packages/mlflow/pyfunc/utils/data_validation.py:168: UserWarning: \u001b[33mAdd type hints to the `predict` method to enable data validation and automatic signature inference during model logging. Check https://mlflow.org/docs/latest/model/python_model.html#type-hint-usage-in-pythonmodel for more details.\u001b[0m\n",
      "  color_warning(\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "import subprocess\n",
    "\n",
    "class MLXModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        # Store the model path, which will be used later in the prediction step\n",
    "        self.model_path = context.artifacts[\"model\"]\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Log the input type\n",
    "        print(f\"Input type: {type(model_input)}\")\n",
    "        print(f\"Input data: {model_input}\")\n",
    "\n",
    "        # Check if the input is a DataFrame and extract the 'inputs' column\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            if 'inputs' in model_input.columns:\n",
    "                # Loop through the DataFrame and process each row\n",
    "                responses = []\n",
    "                for comment in model_input['inputs']:\n",
    "                    prompt = self.prompt_template(comment)\n",
    "\n",
    "                    command = [\n",
    "                        'python', '-m', 'mlx_lm.generate',\n",
    "                        '--model', self.model_path,  # Use the stored model path from load_context\n",
    "                        '--max-tokens', '512',\n",
    "                        '--prompt', prompt\n",
    "                    ]\n",
    "\n",
    "                    # Run the command to generate the response\n",
    "                    result = subprocess.run(command, capture_output=True, text=True)\n",
    "                    responses.append(result.stdout)\n",
    "                return responses\n",
    "            else:\n",
    "                raise ValueError(\"Expected 'inputs' column in the DataFrame.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input type: {type(model_input)}\")\n",
    "\n",
    "    def prompt_template(self, comment):\n",
    "        # Custom prompt template\n",
    "        instructions_string = \"\"\"MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always providing the students steps to solve the question before providing the answer. \\\n",
    "It reacts to feedback aptly and ends responses with its signature 'MathGPT'. \\\n",
    "MathGPT will tailor the length of its responses to match the student's questions, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\n",
    "thus keeping the interaction natural and engaging.\n",
    "Please respond to the following comment.\n",
    "\"\"\"\n",
    "        return f'''<s>[INST] {instructions_string} \\n{comment} \\n[/INST]\\n'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log the model in MLFlow and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 11:30:13 INFO mlflow.pyfunc: Validating input example against model signature\n",
      "Downloading artifacts: 100%|██████████| 25/25 [00:01<00:00, 23.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "0  [INST] MathGPT, functioning as a virtual Math ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 32/32 [01:40<00:00,  3.14s/it] \n",
      "2025/03/06 11:32:36 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "2025/03/06 11:32:36 INFO mlflow.models.evaluation.evaluators.default: Computing model predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "0  MathGPT, functioning as a virtual Math chatbot...\n",
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "1  MathGPT, functioning as a virtual Math chatbot...\n",
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "2  MathGPT, functioning as a virtual Math chatbot...\n",
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "3  MathGPT, functioning as a virtual Math chatbot...\n",
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "4  MathGPT, functioning as a virtual Math chatbot...\n",
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "5  MathGPT, functioning as a virtual Math chatbot...\n",
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "6  MathGPT, functioning as a virtual Math chatbot...\n",
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "7  MathGPT, functioning as a virtual Math chatbot...\n",
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "8  MathGPT, functioning as a virtual Math chatbot...\n",
      "Input type: <class 'pandas.core.frame.DataFrame'>\n",
      "Input data:                                               inputs\n",
      "9  MathGPT, functioning as a virtual Math chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/06 11:36:42 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/03/06 11:36:46 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/03/06 11:36:46 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 1/1 [00:04<00:00,  4.72s/it]\n",
      "2025/03/06 11:36:52 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2025/03/06 11:36:52 WARNING mlflow.models.evaluation.utils.metric: Did not log metric 'toxicity' at index 1 in the `extra_metrics` parameter because it returned None.\n",
      "100%|██████████| 10/10 [00:04<00:00,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mlflow.models.evaluation.base.EvaluationResult object at 0x2aabcd1d0>\n",
      "🏃 View run handsome-toad-685 at: http://127.0.0.1:5000/#/experiments/377793575770949502/runs/37b38484b3494dc8902ac5d7ae166847\n",
      "🧪 View experiment at: http://127.0.0.1:5000/#/experiments/377793575770949502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mlflow.metrics import latency\n",
    "from mlflow.metrics.genai import answer_correctness\n",
    "import time\n",
    "import mlflow\n",
    "signature = mlflow.models.infer_signature(\n",
    "    model_input=pd.DataFrame({\n",
    "    'inputs': [\n",
    "        \"[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always providing steps before the answer. ...\",\n",
    "    ]\n",
    "}),\n",
    "    model_output=[\"==========\\nLet's solve that!  \\n\\nTo find the answer, we simply add the numbers together: 4 + 4 = 8. \\n\\nSo, 4 + 4 = 8. \\n\\nMathGPT \\n\\n==========\\nPrompt: 119 tokens, 102.358 tokens-per-sec\\nGeneration: 49 tokens, 15.268 tokens-per-sec\\nPeak memory: 1.713 GB\\n\",\n",
    " \"==========\\nTo find the derivative of x², we use the power rule of differentiation.  \\n\\nThe power rule states that the derivative of x<sup>n</sup> is nx<sup>n-1</sup>.\\n\\nApplying this to x²:\\n\\n*  The derivative of x² is 2x<sup>2-1</sup> \\n*  Simplifying, we get 2x<sup>1</sup> \\n*  Therefore, the derivative of x² is **2x**. \\n\\n\\nMathGPT \\n\\n==========\\nPrompt: 120 tokens, 118.289 tokens-per-sec\\nGeneration: 99 tokens, 18.342 tokens-per-sec\\nPeak memory: 1.714 GB\\n\"],\n",
    ")\n",
    "input_example = pd.DataFrame({\n",
    "    'inputs': [\n",
    "        \"[INST] MathGPT, functioning as a virtual Math chatbot, communicates in clear, accessible language, always providing steps before the answer. ...\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "# The evaluation process remains the same\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "mlflow.set_experiment(\"LLM Evaluation\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    logged_model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",  # Path where the model will be stored in MLflow\n",
    "        python_model=MLXModelWrapper(),  # Your custom model wrapper\n",
    "        artifacts={\"model\": \"/Users/wongyenchik/Documents/GitHub/finetune-mlx-math-model/my-model\"},  # The path to your saved MLX model\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "\n",
    "    # Use the function to evaluate the model\n",
    "    results = mlflow.evaluate(\n",
    "        logged_model_info.model_uri,\n",
    "        data=eval_data[:10],\n",
    "        targets=\"ground_truth\",\n",
    "        model_type=\"question-answering\",\n",
    "        extra_metrics=[\n",
    "            answer_correctness(),\n",
    "            latency(),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Example output DataFrame for inspection\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 52711.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Hello! 👋  How can I help you today? 😊 \n",
      "\n",
      "==========\n",
      "Prompt: 11 tokens, 21.443 tokens-per-sec\n",
      "Generation: 15 tokens, 16.654 tokens-per-sec\n",
      "Peak memory: 2.946 GB\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "model, tokenizer = load(\"yenchik/mlx-gemma-2-2b-it-math\")\n",
    "\n",
    "prompt = \"Helo\"\n",
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
